# Cursor Learning Overlay

Transform idle time during Cursor AI agent execution into productive microlearning opportunities. This extension creates a fullscreen learning overlay that appears when Cursor's AI agent is running, turning your idle time into engaging educational content.

## Features

- **Automatic Fullscreen Overlay**: Automatically takes over your Cursor IDE when you're idle
- **Dynamic Content Generation**: Fresh fun facts and quizzes generated by LLM every 30 seconds
- **Seamless Integration**: Shows inside Cursor IDE, not in browser - blocks the entire interface
- **Local LLM Integration**: Uses Ollama for content generation (no external API keys needed)
- **Smart Content**: Avoids repeating questions and personalizes based on your interests
- **Auto-Hide**: Instantly disappears when you start typing or moving your cursor

## Prerequisites

1. **Ollama**: Install and run Ollama locally
   ```bash
   # Install Ollama (macOS)
   brew install ollama
   
   # Start Ollama
   ollama serve
   
   # Pull a model (recommended: wizardlm2:latest)
   ollama pull wizardlm2:latest
   ```

2. **Cursor IDE**: This extension is specifically designed for Cursor IDE

## Installation

1. Clone this repository
2. Install dependencies:
   ```bash
   npm install
   ```
3. Compile the extension:
   ```bash
   npm run compile
   ```
4. Press `F5` in VS Code to run the extension in a new Extension Development Host window

## Usage

### How It Works

1. **Completely Automatic**: The extension starts monitoring immediately when Cursor loads
2. **Idle Detection**: Shows learning overlay when you're idle for 5+ seconds
3. **Dynamic Content**: Generates fresh fun facts and quizzes every 30 seconds
4. **Seamless Experience**: Blocks Cursor IDE completely - no manual interaction needed
5. **Auto-Hide**: Disappears instantly when you start typing or moving your cursor

### Manual Controls (Optional)

- **Toggle Overlay**: `Ctrl+Shift+P` → "Learning: Toggle Learning Overlay"
- **Test Mode**: `Ctrl+Shift+P` → "Learning: Simulate Agent (Test)" to manually trigger

### Configuration

Open Settings (`Ctrl+,`) and search for "Cursor Learning Overlay" to configure:

- **Topics**: Choose your learning topics (JavaScript, TypeScript, React, etc.)
- **Ollama URL**: Default is `http://localhost:11434`
- **Model**: Default is `wizardlm2:latest`
- **Enabled**: Toggle the extension on/off

### Learning Flow

1. **Fun Fact**: See an interesting fact about your chosen topic
2. **Test Me**: Click to reveal a multiple choice question
3. **Answer**: Select your answer
4. **Explanation**: See the correct answer and explanation
5. **Next**: Continue with more questions or close the overlay

## How It Works

1. **Agent Detection**: Monitors for AI agent activity (currently simulated with idle detection)
2. **Content Generation**: Uses Ollama to generate personalized learning content
3. **Smart Caching**: Avoids repeating questions and tracks your learning history
4. **Seamless Integration**: Overlay appears/disappears automatically

## Development

### Project Structure

```
src/
├── extension.ts              # Main extension entry point
├── learningOverlayManager.ts # Manages the fullscreen overlay
├── ollamaService.ts          # Handles Ollama API integration
├── localStorage.ts           # Manages user preferences and history
└── agentEventDetector.ts     # Detects AI agent lifecycle events
```

### Building

```bash
# Compile TypeScript
npm run compile

# Watch for changes
npm run watch
```

### Testing

1. Run the extension in development mode (`F5`)
2. Use the "Simulate Agent (Test)" command to test the overlay
3. Check the console for debug information

## Future Enhancements

- **Real Agent Integration**: Connect to actual Cursor AI agent events
- **Cloud Sync**: Sync learning progress across devices
- **Gamification**: Add streaks, XP, and achievements
- **More Content Types**: Code snippets, coding challenges, etc.
- **Analytics**: Track learning progress and topic mastery

## Troubleshooting

### Ollama Not Running
- Make sure Ollama is installed and running: `ollama serve`
- Check the Ollama URL in settings (default: `http://localhost:11434`)
- Verify the model is available: `ollama list`

### Overlay Not Appearing
- Check if the extension is enabled in settings
- Try the manual toggle command
- Check the console for error messages

### Content Not Generating
- Verify Ollama is running and accessible
- Check your internet connection (for model downloads)
- Try a different model in settings

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## License

MIT License - see LICENSE file for details
